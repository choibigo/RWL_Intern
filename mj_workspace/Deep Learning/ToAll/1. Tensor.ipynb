{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **모두를 위한 딥러닝**\n",
    "slide\n",
    "https://drive.google.com/drive/folders/1qVcF8-tx9LexdDT-IY6qOnHc8ekDoL03\n",
    "\n",
    "git\n",
    "https://github.com/deeplearningzerotoall/PyTorch?tab=readme-ov-file\n",
    "\n",
    "youtube\n",
    "https://www.youtube.com/watch?v=7eldOrjQVi0&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Tensor**\n",
    "\n",
    "**batch size = 세로** <br>\n",
    "**wide,length = 가로**\n",
    "\n",
    "1. 2D Tensor\n",
    "\n",
    "- 주로 MLP의 입력이나 Simple Linear Regression에 사용\n",
    "\n",
    "- (Batch Size, Figure Size)\n",
    "- $[ \\text{B}, \\text{F} ]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 3D Tensor \n",
    "\n",
    "- [CV] 이미지 데이터를 다룰 때 주로 사용 (Channel-First)\n",
    "    - [$batch ,width ,height$]\n",
    "- [NLP] 자연어 처리 주로\n",
    "    - [$batch ,length ,dim$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 3D Tensor\n",
    "\n",
    "- 이미지 데이터를 다룰 때 주로 사용 (Channel-First)\n",
    "\n",
    "\n",
    "- (Batch Size, Channels, Height, Width)\n",
    "- $B \\times C \\times H \\times W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is just like NumPy - 1D\n",
      "-------------------------------\n",
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print('PyTorch is just like NumPy - 1D')\n",
    "print('-------------------------------')\n",
    "\n",
    "ft = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(ft)\n",
    "print(ft.dim())  # rank\n",
    "print(ft.shape)  # shape\n",
    "print(ft.size()) # shape\n",
    "print(ft[0], ft[1], ft[-1]) # Element\n",
    "print(ft[2:5], ft[4:-1])    # Slicing\n",
    "print(ft[:2], ft[3:])       # Slicing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n",
      "tensor([[3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting\n",
    "m1 = torch.FloatTensor([[3, 3]])  #1x2 mat\n",
    "m2 = torch.FloatTensor(([2, 2]))\n",
    "print(m1 + m2)\n",
    "\n",
    "m1 = torch.FloatTensor([[1, 2]])  #1x2 mat\n",
    "m2 = torch.FloatTensor([2])  # scalar-> 1x2 mat (Broadcasting)\n",
    "print(m1 + m2)\n",
    "\n",
    "m1 = torch.FloatTensor([[1, 2]])  #1x2 mat -> 2x2 mat  (Broadcasting)\n",
    "m2 = torch.FloatTensor([2, 4])  # 2x1 mat -> 2x2 mat   (Broadcasting)\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 10.,  20.],\n",
      "        [300., 400.]])\n",
      "tensor([[ 10.,  20.],\n",
      "        [300., 400.]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication (내적) -> `matmul`\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1.matmul(m2)) # 2 x 1 (행렬곱)\n",
    "\n",
    "# elemnet-wise Multiplication -> `mul`\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[10], [100]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1 * m2) # 2 x 2\n",
    "print(m1.mul(m2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n",
      "Can only calculate the mean of floating types. Got Long instead.\n",
      "\n",
      "---------\n",
      "Mean - 2D\n",
      "---------\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "# Mean (평균) - 1D\n",
    "t = torch.FloatTensor([1, 2])\n",
    "print(t.mean())\n",
    "\n",
    "# Can't use mean() on integers\n",
    "t = torch.LongTensor([1, 2])\n",
    "try:\n",
    "    print(t.mean())\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "\n",
    "print()\n",
    "print('---------')\n",
    "print('Mean - 2D')\n",
    "print('---------')\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "\n",
    "print(t.mean())\n",
    "print(t.mean(dim=0)) #세로행 mean\n",
    "print(t.mean(dim=1)) # 가로행 Mean\n",
    "print(t.mean(dim=-1)) # 마지막행=가로행 Mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "\n",
    "print(t.sum())\n",
    "print(t.sum(dim=0)) # 세로행 sum\n",
    "print(t.sum(dim=1)) # 가로행\n",
    "print(t.sum(dim=-1)) # 마지막행=가로행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(4.)\n",
      "(tensor([3., 4.]), tensor([1, 1]))\n",
      "Max:  tensor([3., 4.])\n",
      "Argmax:  tensor([1, 1])\n",
      "(tensor([2., 4.]), tensor([1, 1]))\n",
      "(tensor([2., 4.]), tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# max(최댓값), Argmax(최댓값의 위치 반환)\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "\n",
    "print(t.max()) # Returns one value: max\n",
    "print(t.max(dim=0)) # Returns two values: max(index=0) and argmax(index=1)\n",
    "print('Max: ', t.max(dim=0)[0])    # max(index=0)\n",
    "print('Argmax: ', t.max(dim=0)[1]) # argmax(index=1)\n",
    "\n",
    "print(t.max(dim=1))\n",
    "print(t.max(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n",
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# View(Reshape) 중요\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft.shape)   # 2x2x3 mat\n",
    "\n",
    "# -1이 들어간 dim은 자동 결정시킴\n",
    "print(ft.view([-1, 3]))     # 2x2x3 mat -> 4x3 mat\n",
    "print(ft.view([-1, 3]).shape)\n",
    "\n",
    "print(ft.view([-1, 1, 3])) # 1,2 dim은 1,3차원으로 0dim은 자동으로해봐\n",
    "print(ft.view([-1, 1, 3]).shape) # -> 4x1x3 mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Squeeze (`차원 제거`를 통해 개수를 맞추기 위해 사용)\n",
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape) # 3x1 mat\n",
    "\n",
    "print(ft.squeeze())  # 3x1 -> (3,)\n",
    "print(ft.squeeze().shape) # dim 1 제거\n",
    "\n",
    "# Unsqueeze (차원 추가)\n",
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(ft.shape) # (3,)\n",
    "\n",
    "print(ft.unsqueeze(0))  # 0번째(맨앞)에 차원 추가\n",
    "print(ft.unsqueeze(0).shape) # (3,) -> 1x3 mat\n",
    "\n",
    "print(ft.unsqueeze(1)) # 1번째(뒤)에 차원 추가\n",
    "print(ft.unsqueeze(1).shape) # (3,) -> 3x1 mat\n",
    "\n",
    "print(ft.unsqueeze(-1))  # 마지막 = 1번째(뒤)\n",
    "print(ft.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Scatter (One-hot Encoding)\n",
    "# one-hot encoding = 하나만 1이고 나머지 0 (loss구할때 정답label처리시 썼었음)\n",
    "lt = torch.LongTensor([[0], [1], [2], [0]])\n",
    "print(lt) # 4x1 mat\n",
    "\n",
    "one_hot = torch.zeros(4, 3) # batch_size = 4, classes = 3\n",
    "# lt -> 0번은 0에, 1번 행은 1에 ~~...\n",
    "one_hot.scatter_(1, lt, 1) # scatter(dim=1(가로방향), lt(인덱스위치), 1채워라)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Casting (타입 변환)\n",
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(lt)\n",
    "print(lt.float())\n",
    "\n",
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt) \n",
    "print(bt.long())\n",
    "print(bt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate (붙이기)\n",
    "x = torch.FloatTensor([[1,2], [3, 4]])\n",
    "y = torch.FloatTensor([[5,6], [7, 8]])\n",
    "\n",
    "print(torch.cat([x,y], dim = 0))\n",
    "print(torch.cat([x,y], dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking (Concatenate를 편하게)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "print(torch.stack([x, y, z]))\n",
    "print(torch.stack([x, y, z], dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# ones and zeros (모양 본뜨기)\n",
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)\n",
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n",
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "# zip (병렬반복) -> 데어티와 정답 레이블을 쌍으로 묶어 처리할 때 사용\n",
    "for x, y in zip([1, 2, 3], [4, 5, 6]):\n",
    "    print(x, y)\n",
    "\n",
    "for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n",
    "    print(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-place Operation\n",
    "scatter_ 처럼 함수 이름 뒤에 **언더바(_)**가 붙으면 In-place 연산이라고 해서, 새로운 텐서를 만드는 게 아니라 자기 자신을 직접 수정합니다. 메모리를 아낄 수 있죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(t.mul(2.))\n",
    "print(t)\n",
    "\n",
    "print(t.mul_(2.))  # in-place operation\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
