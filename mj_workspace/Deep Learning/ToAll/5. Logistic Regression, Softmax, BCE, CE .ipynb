{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹**\n",
    "slide\n",
    "https://drive.google.com/drive/folders/1qVcF8-tx9LexdDT-IY6qOnHc8ekDoL03\n",
    "\n",
    "git\n",
    "https://github.com/deeplearningzerotoall/PyTorch?tab=readme-ov-file\n",
    "\n",
    "youtube\n",
    "https://www.youtube.com/watch?v=7eldOrjQVi0&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‚ PyTorch ì†ì‹¤ í•¨ìˆ˜(Loss Function) ë¹„êµ\n",
    "\n",
    "| íŠ¹ì§• | Binary Cross Entropy (BCE) | Cross Entropy (CE) |\n",
    "| :--- | :--- | :--- |\n",
    "| **ë¶„ë¥˜ ì¢…ë¥˜** | **ì´ì§„ ë¶„ë¥˜** (Yes/No, 0 ë˜ëŠ” 1) | **ë‹¤ì¤‘ ë¶„ë¥˜** (3ê°œ ì´ìƒì˜ í´ë˜ìŠ¤) |\n",
    "| **ëª¨ë¸ ì¶œë ¥ ë…¸ë“œ** | **1ê°œ** (0~1 ì‚¬ì´ì˜ í™•ë¥ ê°’) | **í´ë˜ìŠ¤ ê°œìˆ˜ë§Œí¼** (ê° í´ë˜ìŠ¤ì˜ ì ìˆ˜) |\n",
    "| **í•„ìš” í™œì„±í™” í•¨ìˆ˜** | **Sigmoid** ì§ì ‘ ì ìš© í•„ìš” | í•„ìš” ì—†ìŒ (**Softmax** ìë™ í¬í•¨) |\n",
    "| **íƒ€ê²Ÿ ë°ì´í„°($y$)** | $0$ ë˜ëŠ” $1$ (ì‹¤ìˆ˜í˜• `FloatTensor`) | ì •ë‹µ í´ë˜ìŠ¤ì˜ **ì¸ë±ìŠ¤** (`LongTensor`) |\n",
    "| **PyTorch í•¨ìˆ˜** | `F.binary_cross_entropy` | `F.cross_entropy` |\n",
    "\n",
    "---\n",
    "\n",
    "1. **BCE**ë¥¼ ì“¸ ë•ŒëŠ” ëª¨ë¸ ë§ˆì§€ë§‰ì— `nn.Sigmoid()`ë¥¼ ê¼­ ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
    "2. **CE**ë¥¼ ì“¸ ë•ŒëŠ” ëª¨ë¸ ë§ˆì§€ë§‰ì— `nn.Softmax()`ë¥¼ ë¶™ì´ì§€ **ì•ŠìŠµë‹ˆë‹¤**. (í•¨ìˆ˜ ë‚´ë¶€ì— í¬í•¨ë˜ì–´ ìˆìŒ)\n",
    "3. ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•´ **BCE**ë„ ì‹œê·¸ëª¨ì´ë“œë¥¼ ë–¼ê³  `F.binary_cross_entropy_with_logits`ë¥¼ ì“°ëŠ” ê²ƒì´ ê¶Œì¥ë˜ê¸°ë„ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Logistic Classification (Regression)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Sigmoid**\n",
    "- ë‹¨ìˆœ Linear Regressionì€ ê²°ê³¼ê°’ì´ ë¬´í•œëŒ€ê¹Œì§€ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤.\n",
    "- ê·¸ëŸ¬ë‚˜ í•©/ë¶ˆ ê°™ì€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ë ¤ë©´ 0ê³¼ 1ì‚¬ì´ì˜ í™•ë¥ ì´ í•„ìš”í•˜ë‹¤.\n",
    "    - ë”°ë¼ì„œ, Sigmoid í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$\n",
    "\n",
    "\n",
    "**BCE (Binary Cross Entropy)**\n",
    "- ì •ë‹µì´ 0ì•„ë‹ˆë©´ 1ì´ë¯€ë¡œ MSEëŒ€ì‹  `log`ë¥¼ í™œìš©í•œ BCE ì‚¬ìš©\n",
    "    - 0~1ì‚¬ì´ ê°’ì´ ë‚˜ì˜¤ë¯€ë¡œ MSEëŠ” ì˜¤ì°¨ì œê³±ì´ë¯€ë¡œ ì•„ë¬´ë¦¬ì»¤ë„ Lossê°€ 1 ì´í•˜ì´ë‹¤\n",
    "    - BCEëŠ” logë¡œ ì¸í•´ ê°’ì´ í›¨ì”¬ í¬ê²Œë‚˜ì˜´\n",
    "    \n",
    "    $$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8]) torch.Size([759, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. nn.Moduleë¡œ ëª¨ë¸ ìƒì„±\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(8, 1) # input:8, output:1\n",
    "        self.sigmoid = nn.Sigmoid()   # sigmoid: 0~1 ì‚¬ì´ í™•ë¥ ë¡œ ë³€í™˜\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "# 2. ë°ì´í„° ë° ëª¨ë¸ ì´ˆê¸°í™”\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "model = BinaryClassifier()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.721252 Accuracy 38.08%\n",
      "Epoch   10/100 Cost: 0.581246 Accuracy 66.40%\n",
      "Epoch   20/100 Cost: 0.544663 Accuracy 72.60%\n",
      "Epoch   30/100 Cost: 0.523263 Accuracy 75.63%\n",
      "Epoch   40/100 Cost: 0.509751 Accuracy 76.42%\n",
      "Epoch   50/100 Cost: 0.500702 Accuracy 76.68%\n",
      "Epoch   60/100 Cost: 0.494369 Accuracy 76.94%\n",
      "Epoch   70/100 Cost: 0.489784 Accuracy 76.81%\n",
      "Epoch   80/100 Cost: 0.486375 Accuracy 76.15%\n",
      "Epoch   90/100 Cost: 0.483786 Accuracy 76.15%\n",
      "Epoch  100/100 Cost: 0.481782 Accuracy 76.42%\n"
     ]
    }
   ],
   "source": [
    "# 3. í•™ìŠµ ë£¨í”„\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # fowardProp, BCE ê³„ì‚°\n",
    "    hypothesis = model(x_train)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train) # BCE ì‚¬ìš©\n",
    "\n",
    "    # backprop + step\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # ë¡œê·¸ ì¶œë ¥\n",
    "    if epoch % 10 == 0:\n",
    "        # ì •í™•ë„ ê³„ì‚° ê³¼ì •\n",
    "        prediction = hypothesis >= torch.tensor([0.5])     # 0.5 ë„˜ìœ¼ë©´ True(1)\n",
    "        correct_prediction = prediction.float() == y_train # ì •ë‹µê³¼ ë¹„êµ\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        \n",
    "        print(f'Epoch {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f} Accuracy {accuracy*100:2.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Softmax Classification\n",
    "\n",
    "**Softmax**\n",
    "- ì–´ë–¤ ìˆ«ì ë²¡í„°ë¥¼ **í™•ë¥  ë¶„í¬**ë¡œ ë³€í™˜\n",
    "- ëª¨ë“  ì¶œë ¥ê°’ì€ 0~1 ì‚¬ì´ì´ë©°, ì´í•©ì€ í•­ìƒ **1**ì´ ë¨\n",
    "\n",
    "$$P(class = i) = \\frac{e^i}{\\sum e^j}$$\n",
    "\n",
    "### Cross Entropy\n",
    "- ì‹¤ì œ ì •ë‹µ ë¶„í¬($P$)ì™€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥  ë¶„í¬($Q$) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” **ì†ì‹¤ í•¨ìˆ˜**.\n",
    "\n",
    "$$H(P, Q) = -\\sum_{x \\in \\mathcal{X}} P(x) \\log Q(x)$$\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum -y \\log(\\hat{y})$$\n",
    "- $N$: ë°ì´í„°ì˜ ê°œìˆ˜ (Batch size)\n",
    "- $y$: ì‹¤ì œ ì •ë‹µ (0 ë˜ëŠ” 1)\n",
    "- $\\hat{y}$: ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ \n",
    "- ë°ì´í„° ì—¬ëŸ¬ ê°œ(Nê°œ)ì˜ ì˜¤ì°¨ë¥¼ ë‹¤ ë”í•´ì„œ í‰ê· "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ìƒì„±\n",
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "\n",
    "# y_data: í´ë˜ìŠ¤ ì¸ë±ìŠ¤ (0, 1, 2 ì¤‘ í•˜ë‚˜)\n",
    "# F.cross_entropyë¥¼ ì“¸ ë•ŒëŠ” One-hotì´ ì•„ë‹Œ ì¸ë±ìŠ¤(LongTensor)ë¥¼ ë„£ì–´ì•¼ í•¨\n",
    "y_data = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "# F.cross_entropyë¥¼ ì“¸ë•ŒëŠ” labelê°’ì´ `ì •ìˆ˜`\n",
    "y_train = torch.LongTensor(y_data)\n",
    "\n",
    "# optimizer ì„¤ì •\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 loss: 0.115210\n",
      "Epoch  100/1000 loss: 0.112222\n",
      "Epoch  200/1000 loss: 0.109382\n",
      "Epoch  300/1000 loss: 0.106678\n",
      "Epoch  400/1000 loss: 0.104101\n",
      "Epoch  500/1000 loss: 0.101643\n",
      "Epoch  600/1000 loss: 0.099295\n",
      "Epoch  700/1000 loss: 0.097051\n",
      "Epoch  800/1000 loss: 0.094904\n",
      "Epoch  900/1000 loss: 0.092847\n",
      "Epoch 1000/1000 loss: 0.090876\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) ê³„ì‚°\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost ê³„ì‚°\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # costë¡œ H(x) ê°œì„ \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20ë²ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} loss: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
