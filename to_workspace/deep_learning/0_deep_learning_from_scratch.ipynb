{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 계획\n",
        "\n",
        "- [x] 딥러닝 공부 계획\n",
        "- [x] 강의 조사\n",
        "\n",
        "# 밑바닥부터 시작하는 딥러닝 1\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"image-1.png\" width=\"300\">\n",
        "</div>\n",
        "\n",
        "# CHAPTER 2 퍼셉트론\n",
        "\n",
        "- 퍼셉트론은 다수의 신호를 입력 받아 하나의 신호, `0` 또는 `1`을 내보냄.\n",
        "\n",
        "![alt text](image.png)\n",
        "\n",
        "- 그림에서 보이는 하나의 원이 하나의 **노드**, 다른말로 **뉴런**임.\n",
        "- w는 각각의 weight(가중치)이다.\n",
        "\n",
        "이를 수식으로 나타내면 아래와 같으며\n",
        "\n",
        "![alt text](image-2.png)\n",
        "\n",
        "- 여기서 이 노드 또는 뉴런의 활성 임계값을 **세타**로 표기함.\n",
        "\n",
        "- 하지만 이 세타는 나중에 가면 잘 안씀, 수학적으로 이 임계값이 결국 bias(b로 표기)의 역할이 똑같아서 그냥 교체 됨. 오히려 더 심플함.\n",
        " - 즉, bias는 뉴런이 얼마나 쉽게 활성화 되느냐를 가르는 용도. 적어도 퍼셉트론에서는.\n",
        "\n",
        "![alt text](image-4.png)\n",
        "\n",
        "- 이런 퍼셉트론 뉴런을 여럿 이용해서 보다 복잡한 논리도 구현할 수 있음. XOR 같은.\n",
        "\n",
        "![alt text](image-5.png)\n",
        "\n",
        "# CHAPTER 3 신경망\n",
        "\n",
        "- 신경망은 위 퍼셉트론에서 더 응용된 개념이. 어찌보면 더 포괄적인 개념이기도하고.\n",
        "\n",
        "- 신경망은 Input, Hidden Output layer으로 구성됨.\n",
        "\n",
        "![alt text](image-6.png)\n",
        "\n",
        "(이걸 문헌에 따라 2개의 층이라 가졌다하기도 하고, 3개의 층이라는 말도 있음. 이 교재에서는 2층이라고 말함.)\n",
        "\n",
        "- bias를 포함하여 아래 그림 처럼 보여주기도 함.\n",
        "\n",
        "![alt text](image-7.png)\n",
        "\n",
        "### 활성화 함수\n",
        "\n",
        "- 활성화 함수: 입력 신호의 총합을 출력 신호로 변환하는 함수.\n",
        "  - 노드 안에서 작동한다고 생각하면 쉬움.\n",
        "  - 퍼셉트론 처럼 꼮 임계치를 넘겨야 활성화가 되고말고를 결정하는 것은 아니고, 활성화 함수마다 출력 방식이 다름.\n",
        "\n",
        "![alt text](image-8.png)\n",
        "\n",
        "- 활성화 함수는 주로 `h()`으로 표현하며, 인풋으로 input 총합을 알맞게 아웃풋으로 만들어냄. 위 예시는 퍼셉트론과 같은 0과 1만을 출력하는 계단함수임. 당연하지만 이 계단함수를 실제로 잘 쓰지는 않음.\n",
        "\n",
        "> 참고로 계단말고 인풋을 아웃풋에 그대로 전달하는 방식도 있긴함. 하지만 쓰이지는 않음. 왜냐하면 그렇게 하면 시스템이 선형밖에 안됨. 즉, 여러개의 행렬을 붙인다는 말이 되는거고, 그 말은 사실 하나의 행렬을 이용하는 것과 같음. 이는 아무리 많은 레이러를 쌓아도 결국 하나의 선형변환 밖에 안된다는 소리.\n",
        "\n",
        "- 아래 처럼 시그모이드 함수를 이용하기도 함. 이는 명백한 비선형이고, 0 부근에서 확실한 고저차가 있어서 좋음. 미분도 가능하고.\n",
        "\n",
        "![alt text](image-9.png)\n",
        "\n",
        "참고로 `exp(-x)`는 $e^{-x}$를 표현한 것.\n",
        "\n",
        "- 파이썬으로 쓰면 아래 처럼 됨."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'base (Python 3.13.11)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CHAPTER 4 신경망 학습\n",
        "\n",
        "# CHAPTER 5 오차역전파법\n",
        "\n",
        "# CHAPTER 6 학습 관련 기술들\n",
        "\n",
        "# CHAPTER 7 합성곱 신경망(CNN)\n",
        "\n",
        "# CHAPTER 8 딥러닝"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
